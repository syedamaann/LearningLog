The journey started with loading a pretraining dataset from Hugging Face and combining it with some freshly scraped data. Then cleaning it up by removing duplicates and non-English text, After this, I reshaped and tokenized the dataset, making it ready for model training. This was a crucial step to ensure our dataset was in tip-top shape for the pretraining process.

Next, I set up a 16-layer model and initialized it with random weights. To give it a good starting point, I copied the bottom 8 and top 8 layers from a pretrained 12-layer model. This kind of smart initialization helps in stabilizing the training process and often leads to better performance. After configuring the training arguments and parsing custom settings, I set the default BF16 options to false for devices that donâ€™t support it, ensuring compatibility across different hardware setups.

For the training phase, I set up a trainer and implemented a callback to log loss values, which is super helpful for monitoring progress. I also fixed a few bugs along the way, like resolving a multiprocessing error and avoiding directory reading issues. Finally, I added functions for evaluation, saved checkpoints, and implemented inference capabilities.
